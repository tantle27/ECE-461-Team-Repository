# Unit Tests for ACME AI/ML Model Evaluation System

This directory contains comprehensive unit tests for the ACME Corporation AI/ML Model Evaluation System.

## ðŸ“‹ Test Structure

```
tests/
â”œâ”€â”€ __init__.py                      # Test package initialization
â”œâ”€â”€ test_base_metric.py             # Tests for BaseMetric abstract class
â”œâ”€â”€ test_metric_eval.py             # Tests for MetricEval orchestrator
â”œâ”€â”€ test_community_rating_metric.py # Tests for CommunityRatingMetric
â””â”€â”€ test_integration.py             # Integration tests for complete system
```

## ðŸš€ Running Tests

### Prerequisites
First, install the testing dependencies:
```bash
pip install pytest pytest-cov pytest-mock
```

### Run All Tests
```bash
# From project root directory
python -m pytest tests/ -v

# Or using the test runner script
python run_tests.py
```

### Run Specific Test Files
```bash
# Run BaseMetric tests
python -m pytest tests/test_base_metric.py -v

# Run MetricEval tests  
python -m pytest tests/test_metric_eval.py -v

# Run Community Rating Metric tests
python -m pytest tests/test_community_rating_metric.py -v

# Run Integration tests
python -m pytest tests/test_integration.py -v
```

### Run Tests with Coverage
```bash
python -m pytest tests/ --cov=src --cov-report=html --cov-report=term-missing
```

## ðŸ“Š Test Coverage

The test suite covers:

### BaseMetric Tests (`test_base_metric.py`)
- âœ… Abstract class cannot be instantiated
- âœ… Concrete implementations work correctly
- âœ… Initialization with different weights
- âœ… String representation
- âœ… Error handling for evaluation failures
- âœ… Variable input handling

### MetricEval Tests (`test_metric_eval.py`)
- âœ… Initialization with metrics and weights
- âœ… Parallel evaluation of multiple metrics
- âœ… Score aggregation with proper weighting
- âœ… Error handling for failing metrics
- âœ… Edge cases (empty inputs, unicode names, etc.)
- âœ… Concurrent execution testing

### CommunityRatingMetric Tests (`test_community_rating_metric.py`)
- âœ… Logarithmic scaling of likes and downloads
- âœ… Handling missing data gracefully
- âœ… Score clamping to [0.0, 1.0] range
- âœ… Realistic scenario testing
- âœ… Mathematical correctness of scoring

### Integration Tests (`test_integration.py`)
- âœ… Complete evaluation pipeline
- âœ… Multiple model evaluation
- âœ… Error handling in full system
- âœ… Weight impact verification
- âœ… Performance with many metrics

## ðŸŽ¯ Test Categories

### Unit Tests
- Test individual components in isolation
- Mock external dependencies
- Fast execution (< 1 second per test)

### Integration Tests  
- Test components working together
- Use real implementations
- May be slower but test real scenarios

### Performance Tests
- Marked with `@pytest.mark.slow`
- Test system scalability
- Can be skipped with: `pytest -m "not slow"`

## ðŸ“ˆ Expected Test Results

When all tests pass, you should see output like:
```
tests/test_base_metric.py::TestBaseMetric::test_base_metric_cannot_be_instantiated PASSED
tests/test_base_metric.py::TestBaseMetric::test_concrete_metric_initialization PASSED
tests/test_metric_eval.py::TestMetricEval::test_metric_eval_initialization PASSED
tests/test_community_rating_metric.py::TestCommunityRatingMetric::test_initialization PASSED
tests/test_integration.py::TestIntegrationMetricsSystem::test_full_evaluation_pipeline PASSED

========================= XX passed in X.XXs =========================
```

## ðŸ› Debugging Failed Tests

### Common Issues
1. **Import Errors**: Ensure you're running from project root and have proper Python path
2. **Missing Dependencies**: Run `pip install -r requirements.txt`
3. **Path Issues**: Check that file paths in tests match your project structure

### Verbose Output
For more detailed output when tests fail:
```bash
python -m pytest tests/ -vvs --tb=long
```

### Running Single Test
To debug a specific test:
```bash
python -m pytest tests/test_base_metric.py::TestBaseMetric::test_initialization -vvs
```

## ðŸ“ Adding New Tests

When adding new metrics or functionality:

1. **Create test file**: `test_your_new_metric.py`
2. **Follow naming convention**: `TestYourNewMetric` class
3. **Test all public methods**: Especially `evaluate()` and `get_description()`
4. **Test edge cases**: Empty inputs, invalid data, etc.
5. **Add integration test**: Show it works with MetricEval

### Test Template
```python
import pytest
from src.metrics.your_metric import YourMetric

class TestYourMetric:
    def test_initialization(self):
        metric = YourMetric()
        assert metric.name == "expected_name"
        assert metric.weight == expected_weight
    
    def test_evaluate_basic(self):
        metric = YourMetric()
        result = metric.evaluate({"test": "data"})
        assert 0.0 <= result <= 1.0
```

## ðŸ”§ Configuration

Test configuration is in `pytest.ini`:
- Test discovery patterns
- Output formatting
- Marker definitions
- Coverage settings (when enabled)

## ðŸ“Š Metrics for Success

A good test suite should have:
- âœ… **High Coverage**: >90% line coverage
- âœ… **Fast Execution**: Most tests < 100ms
- âœ… **Clear Failures**: Easy to understand what went wrong
- âœ… **Comprehensive**: Cover happy path, edge cases, and error conditions
