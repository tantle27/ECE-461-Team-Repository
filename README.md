
# ECE-461 Team Repository

## Team Members
- Trevor
- Jackson  
- Jain Iftesam
- William Ilkanic

## ğŸ“ Project Structure

```
ECE-461-Team-Repository/
â”œâ”€â”€ src/                           # Main source code
â”‚   â”œâ”€â”€ api/                       # API-related modules
â”‚   â”œâ”€â”€ git/                       # Git integration utilities
â”‚   â”œâ”€â”€ metrics/                   # Metric evaluation classes
â”‚   â”‚   â”œâ”€â”€ BaseMetric.py         # Legacy base metric (deprecated)
â”‚   â”‚   â”œâ”€â”€ base_metric.py        # Modern base metric abstract class
â”‚   â”‚   â”œâ”€â”€ bus_factor_metric.py  # Bus factor evaluation
â”‚   â”‚   â”œâ”€â”€ code_quality_metric.py # Code quality assessment
â”‚   â”‚   â”œâ”€â”€ community_rating_metric.py # Community engagement metrics
â”‚   â”‚   â”œâ”€â”€ dataset_availability_metric.py # Dataset accessibility
â”‚   â”‚   â”œâ”€â”€ dataset_quality_metric.py # Dataset quality evaluation
â”‚   â”‚   â”œâ”€â”€ license_metric.py     # License compatibility check
â”‚   â”‚   â”œâ”€â”€ performance_claims_metric.py # Performance validation
â”‚   â”‚   â”œâ”€â”€ ramp_up_time_metric.py # Learning curve assessment
â”‚   â”‚   â”œâ”€â”€ size_metric.py        # Codebase size metrics
â”‚   â”‚   â””â”€â”€ MetricEval.py         # Legacy metric evaluator (deprecated)
â”‚   â”œâ”€â”€ app.py                    # Main application entry point
â”‚   â”œâ”€â”€ handlers.py               # URL and request handlers
â”‚   â”œâ”€â”€ metric_eval.py            # Modern metric evaluation orchestrator
â”‚   â”œâ”€â”€ net_scorer.py             # Weighted scoring system
â”‚   â”œâ”€â”€ repo_context.py           # Repository context data structure
â”‚   â”œâ”€â”€ score_result.py           # Score result data structure
â”‚   â””â”€â”€ url_router.py             # URL routing and parsing
â”œâ”€â”€ tests/                        # Comprehensive test suite
â”‚   â”œâ”€â”€ test_base_metric.py       # BaseMetric abstract class tests
â”‚   â”œâ”€â”€ test_cli_layer.py         # CLI interface tests
â”‚   â”œâ”€â”€ test_community_rating_metric.py # Community metric tests
â”‚   â”œâ”€â”€ test_error_handling.py    # Error handling tests
â”‚   â”œâ”€â”€ test_extended_metrics.py  # Extended metric functionality tests
â”‚   â”œâ”€â”€ test_integration.py       # Integration tests
â”‚   â”œâ”€â”€ test_metric_eval.py       # MetricEval orchestrator tests
â”‚   â”œâ”€â”€ test_net_scorer.py        # NetScorer weighted scoring tests
â”‚   â”œâ”€â”€ test_repo_context.py      # RepoContext data structure tests
â”‚   â”œâ”€â”€ test_routing_layer.py     # URL routing tests
â”‚   â”œâ”€â”€ test_run_install.py       # Installation script tests
â”‚   â”œâ”€â”€ test_run_test.py          # Test runner tests
â”‚   â””â”€â”€ test_url_handlers.py      # URL handler tests
â”œâ”€â”€ .flake8                       # Flake8 linting configuration
â”œâ”€â”€ pytest.ini                   # Pytest configuration
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ run                          # Main executable script
â””â”€â”€ README.md                    # This file
```

## ğŸš€ Getting Started

### Prerequisites
- Python 3.8 or higher
- Git

### Installation

#### Method 1: Using the run script (Recommended)
```bash
# Make the run script executable (Unix/Linux/Mac)
chmod +x run

# If you get "cannot execute: required file not found", convert line endings
dos2unix run

# Install dependencies
./run install
```

#### Method 2: Manual installation
```bash
# Create and activate virtual environment
python3 -m venv venv

# On Windows:
venv\Scripts\activate

# On Unix/Linux/Mac:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Alternative Installation (if ./run install doesn't work)
```bash
python3 -m venv venv
cd venv
source bin/activate  # On Windows: Scripts\activate
cd ..
./run install
```

## ğŸ§ª Running Tests

### Quick Test Run
```bash
# Run all tests
./run test

# Or manually:
python -m pytest tests/ -v
```

### Comprehensive Testing
```bash
# Run all tests with coverage
python -m pytest tests/ --cov=src --cov-report=html --cov-report=term-missing

# Run specific test categories
python -m pytest tests/test_integration.py -v          # Integration tests
python -m pytest tests/test_error_handling.py -v      # Error handling tests
python -m pytest tests/test_metric_eval.py -v         # Metric evaluation tests
```

### Test Coverage Report
After running tests with coverage, open `htmlcov/index.html` in your browser to view detailed coverage reports.

## ğŸ“Š Code Quality

### Linting
The project uses flake8 for code style enforcement:
```bash
# Check all source files
flake8 src/

# Check all test files
flake8 tests/

# Check specific file
flake8 src/app.py
```

### Style Standards
- PEP 8 compliant
- Maximum line length: 88 characters
- Proper import ordering
- No unused variables or imports

## ğŸ“š Usage

### Running the Application
```bash
# Execute the main application
./run

# Or manually:
python src/app.py
```

### Evaluating Models
The system evaluates AI/ML models across multiple metrics:
- **Bus Factor**: Team knowledge distribution
- **Code Quality**: Code maintainability and best practices
- **Community Rating**: User engagement and feedback
- **Dataset Availability**: Data accessibility and documentation
- **Dataset Quality**: Data completeness and accuracy
- **License Compatibility**: Legal compliance assessment
- **Performance Claims**: Benchmark validation
- **Ramp-up Time**: Learning curve estimation
- **Size Metrics**: Codebase complexity assessment

## ğŸ¤ Contributing

1. Follow the existing code style (flake8 compliant)
2. Add comprehensive tests for new features
3. Update documentation as needed
4. Ensure all tests pass before submitting

## ğŸ“„ License

This project is part of the ECE-461 coursework.